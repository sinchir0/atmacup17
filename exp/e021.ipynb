{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path setting\n",
    "EXP_NAME = \"e021-MaxPooling\"\n",
    "MODEL_NAME = \"microsoft/deberta-v3-xsmall\"\n",
    "COMPETITION_NAME = \"atmacup17\"\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "ENV_PATH = \"env_file\"\n",
    "DATASET_NAME = f\"{EXP_NAME}-{MODEL_NAME.split('/')[-1]}\"\n",
    "MODEL_OUTPUT_PATH = f\"trained_models/{EXP_NAME}\"\n",
    "TARGET_COL = \"Recommended IND\"\n",
    "\n",
    "# experiment parameter\n",
    "DEBUG = False\n",
    "TRAINING = True\n",
    "UPLOAD_DATA_TO_S3 = True\n",
    "# UPLOAD_DATA_TO_KAGGLE = True\n",
    "WANDB = True\n",
    "\n",
    "# model parameter\n",
    "TRAINING_MAX_LENGTH = 512\n",
    "INFERENCE_MAX_LENGTH = 512\n",
    "SEED = 42\n",
    "EPOCH = 4\n",
    "LR = 2e-04\n",
    "TRAIN_BS = 8\n",
    "GRAD_ACC_STEP = 128 // TRAIN_BS  # 仮想的なバッチサイズはTRAIN_BS * GRAD_ACC_STEPとなる\n",
    "EVAL_BS = 8\n",
    "NUM_LABELS = 2\n",
    "\n",
    "USE_FOLD = 0  # Fold数は3(0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 30 06:39:41 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P0             39W /  300W |    2469MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    221287      C   /opt/conda/bin/python                        1426MiB |\n",
      "|    0   N/A  N/A    224870      C   /opt/conda/bin/python                        1040MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.14\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shinichiro.saito/atmacup17/exp\n",
      "GCP!\n",
      "/home/shinichiro.saito/atmacup17/data\n",
      "/home/shinichiro.saito/atmacup17/exp\n",
      "GCP!\n",
      "/home/shinichiro.saito/atmacup17/trained_models/e021-MaxPooling\n",
      "/home/shinichiro.saito/atmacup17/exp\n",
      "GCP!\n"
     ]
    }
   ],
   "source": [
    "def resolve_path(base_path: str) -> str:\n",
    "    import os\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    print(cwd)\n",
    "    if cwd == f\"/notebooks\":\n",
    "        print(\"Jupyter Kernel By VSCode!\")\n",
    "        return \"kernel\", f\"/notebooks/{COMPETITION_NAME}/{base_path}\"\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}\":\n",
    "        print(\"nohup!\")\n",
    "        return base_path\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}/{COMPETITION_NAME}/exp\":\n",
    "        print(\"Jupyter Lab!\")\n",
    "        return \"nohup\", f\"../../{base_path}\"\n",
    "    elif cwd == f\"/content\":\n",
    "        print(\"Google Colab!\")\n",
    "        return \"colab\", f\"/content/drive/MyDrive/Kaggle/{COMPETITION_NAME}/{base_path}\"\n",
    "    elif cwd.startswith(\"/home/shinichiro.saito\"):\n",
    "        print(\"GCP!\")\n",
    "        return \"GCP\", f\"/home/shinichiro.saito/{COMPETITION_NAME}/{base_path}\"\n",
    "    else:\n",
    "        raise Exception(\"Unknown environment\")\n",
    "\n",
    "\n",
    "ENV_NAME, DATA_PATH = resolve_path(DATA_PATH)\n",
    "print(DATA_PATH)\n",
    "_, MODEL_OUTPUT_PATH = resolve_path(MODEL_OUTPUT_PATH)\n",
    "print(MODEL_OUTPUT_PATH)\n",
    "_, ENV_PATH = resolve_path(ENV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset_name(dataset_name: str) -> None:\n",
    "    if len(dataset_name) < 6 or len(dataset_name) > 50:\n",
    "        raise Exception(\n",
    "            f\"データセットの文字列は6~50文字にしてください。現在{len(DATASET_NAME)}文字\"\n",
    "        )\n",
    "    if \"_\" in dataset_name:\n",
    "        raise Exception(\"datasetの名称に_の使用は禁止です\")\n",
    "\n",
    "\n",
    "validate_dataset_name(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENV_NAME != \"GCP\":\n",
    "    %pip install -qq polars==1.0.0\n",
    "    %pip install -qq transformers==4.42.3\n",
    "    %pip install -qq sentencepiece==0.2.0\n",
    "    %pip install -qq datasets==2.20.0\n",
    "    %pip install -qq evaluate==0.4.2\n",
    "    %pip install -qq seqeval==1.2.2\n",
    "    %pip install -qq accelerate==0.32.0\n",
    "    %pip install -qq python-dotenv==1.0.1\n",
    "    %pip install -qq wandb==0.17.4\n",
    "    %pip install -qq bitsandbytes==0.43.1\n",
    "    %pip install -qq accelerate==0.32.0\n",
    "    %pip install -qq peft==0.11.1\n",
    "\n",
    "    # formatter\n",
    "    %pip install -qq black isort\n",
    "\n",
    "    %pip install -qq kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import ast\n",
    "import json\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "import wandb\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    Value,\n",
    "    concatenate_datasets,\n",
    "    load_dataset,\n",
    "    ClassLabel,\n",
    ")\n",
    "from typing import Optional, Union, Tuple\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from tokenizers import AddedToken\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import log_loss\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    DebertaV2PreTrainedModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.deberta_v2.modeling_deberta_v2 import (\n",
    "    ContextPooler,\n",
    "    StableDropout,\n",
    "    DebertaV2Model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "NUM_PROC = os.cpu_count()\n",
    "NUM_PROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "assert transformers.__version__ == \"4.42.3\"\n",
    "assert datasets.__version__ == \"2.20.0\"\n",
    "assert evaluate.__version__ == \"0.4.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the same seed to all\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(f\"{ENV_PATH}/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:n67gjx9b) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2212f1bb574d5d9eb2ab677ae37e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">e021-MaxPooling</strong> at: <a href='https://wandb.ai/sinchir0/atmacup17/runs/n67gjx9b' target=\"_blank\">https://wandb.ai/sinchir0/atmacup17/runs/n67gjx9b</a><br/> View project at: <a href='https://wandb.ai/sinchir0/atmacup17' target=\"_blank\">https://wandb.ai/sinchir0/atmacup17</a><br/>Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240830_063427-n67gjx9b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:n67gjx9b). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a63248c1134794a21ad7ca12b15757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111359362225307, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shinichiro.saito/atmacup17/exp/wandb/run-20240830_063944-149dk0r3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinchir0/atmacup17/runs/149dk0r3' target=\"_blank\">e021-MaxPooling</a></strong> to <a href='https://wandb.ai/sinchir0/atmacup17' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinchir0/atmacup17' target=\"_blank\">https://wandb.ai/sinchir0/atmacup17</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinchir0/atmacup17/runs/149dk0r3' target=\"_blank\">https://wandb.ai/sinchir0/atmacup17/runs/149dk0r3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'wandb'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if WANDB:\n",
    "    wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "    wandb.init(project=COMPETITION_NAME, name=EXP_NAME)\n",
    "    REPORT_TO = \"wandb\"\n",
    "else:\n",
    "    REPORT_TO = \"none\"\n",
    "\n",
    "REPORT_TO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{DATA_PATH}/rec_stratified_fold.json\") as f:\n",
    "    label_stratified_fold = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (\n",
    "    pl.read_csv(f\"{DATA_PATH}/train_with_index.csv\")\n",
    "    .join(pl.read_csv(f\"{DATA_PATH}/clothing_master.csv\"), on=\"Clothing ID\", how=\"left\")\n",
    "    .with_columns(\n",
    "        pl.col(\"Title\").fill_null(\"\"),\n",
    "        pl.col(\"Review Text\").fill_null(\"\"),\n",
    "    )\n",
    "    .with_columns(pl.col(\"Rating\") - 1)\n",
    "    .rename({TARGET_COL: \"label\"})\n",
    "    .with_columns(  # foldを追加する\n",
    "        pl.col(\"index\").replace(label_stratified_fold).alias(\"fold\")\n",
    "    )\n",
    ")\n",
    "\n",
    "test = (\n",
    "    pl.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "    .join(pl.read_csv(f\"{DATA_PATH}/clothing_master.csv\"), on=\"Clothing ID\", how=\"left\")\n",
    "    .with_columns(\n",
    "        pl.col(\"Title\").fill_null(\"\"),\n",
    "        pl.col(\"Review Text\").fill_null(\"\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    train = train.head(100)\n",
    "    test = test.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_polars(train)\n",
    "test_dataset = Dataset.from_polars(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/seoyunje/aes-2-custom-deberta-with-different-header\n",
    "class MaxPooling(ContextPooler):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        max_embeddings, _ = last_hidden_state.max(1)\n",
    "        return max_embeddings\n",
    "\n",
    "\n",
    "class MeanPooling(ContextPooler):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = (\n",
    "            attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        )\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)  # ゼロ除算を防ぐ\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "\n",
    "class MeanMaxPooling(ContextPooler):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.mean_pooler = MeanPooling(config)\n",
    "        self.max_pooler = MaxPooling(config)\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        mean_pooling_embeddings = self.mean_pooler(last_hidden_state, attention_mask)\n",
    "        max_pooling_embeddings = self.max_pooler(last_hidden_state, attention_mask)\n",
    "\n",
    "        mean_max_embeddings = torch.cat(\n",
    "            (mean_pooling_embeddings, max_pooling_embeddings), 1\n",
    "        )\n",
    "\n",
    "        return mean_max_embeddings\n",
    "\n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        return self.config.hidden_size * 2\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/361678\n",
    "class AttentionPooling(ContextPooler):\n",
    "    \"\"\"\n",
    "    mean poolingはそれぞれのtokenを等しく足し合わせる。\n",
    "    Attention Poolingは、それぞれのtokenに対する重みの層を新たに学習することで、\n",
    "    tokenに対する重み付けをより詳細に行うことができる。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        in_dim = config.hidden_size\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        w = self.attention(last_hidden_state).float()\n",
    "        w[attention_mask == 0] = float(\"-inf\")\n",
    "        w = torch.softmax(w, 1)\n",
    "        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n",
    "        return attention_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dev.classmethod.jp/articles/huggingface-usage-custom-model/\n",
    "# https://github.com/huggingface/transformers/blob/94b3f544a1f5e04b78d87a2ae32a7ac252e22e31/src/transformers/models/deberta_v2/modeling_deberta_v2.py#L1313\n",
    "\n",
    "\n",
    "class CustomDebertaSequenceClassification(DebertaV2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        num_labels = getattr(config, \"num_labels\", 2)\n",
    "        self.num_labels = num_labels\n",
    "        self.rating_labels = 5\n",
    "\n",
    "        self.deberta = DebertaV2Model(config)\n",
    "        # self.pooler = ContextPooler(config)\n",
    "        self.pooler = MaxPooling(config)\n",
    "        # self.pooler = MeanPooling(config)\n",
    "        # self.pooler = MeanMaxPooling(config)\n",
    "        # self.pooler = AttentionPooling(config)\n",
    "        output_dim = self.pooler.output_dim\n",
    "\n",
    "        self.classifier = nn.Linear(output_dim, num_labels)\n",
    "        self.rating_classifier = nn.Linear(output_dim, self.rating_labels)\n",
    "        drop_out = getattr(config, \"cls_dropout\", None)\n",
    "        drop_out = self.config.hidden_dropout_prob if drop_out is None else drop_out\n",
    "        self.dropout = StableDropout(drop_out)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.deberta.get_input_embeddings()\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        self.deberta.set_input_embeddings(new_embeddings)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        rating: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutput]:\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        outputs = self.deberta(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # CLSの利用\n",
    "        # encoder_layer = outputs[0]\n",
    "        # pooled_output = self.pooler(encoder_layer)\n",
    "        # MaxPooling, MeanPoolingの利用\n",
    "        pooled_output = self.pooler(outputs[\"last_hidden_state\"], attention_mask)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        # Rec\n",
    "        logits = self.classifier(pooled_output)\n",
    "        # Rating\n",
    "        logits_rating = self.rating_classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    # regression task\n",
    "                    loss_fn = nn.MSELoss()\n",
    "                    logits = logits.view(-1).to(labels.dtype)\n",
    "                    loss = loss_fn(logits, labels.view(-1))\n",
    "                elif labels.dim() == 1 or labels.size(-1) == 1:\n",
    "                    label_index = (labels >= 0).nonzero()\n",
    "                    labels = labels.long()\n",
    "                    if label_index.size(0) > 0:\n",
    "                        labeled_logits = torch.gather(\n",
    "                            logits,\n",
    "                            0,\n",
    "                            label_index.expand(label_index.size(0), logits.size(1)),\n",
    "                        )\n",
    "                        labels = torch.gather(labels, 0, label_index.view(-1))\n",
    "                        loss_fct = CrossEntropyLoss()\n",
    "                        loss = loss_fct(\n",
    "                            labeled_logits.view(-1, self.num_labels).float(),\n",
    "                            labels.view(-1),\n",
    "                        )\n",
    "                    else:\n",
    "                        loss = torch.tensor(0).to(logits)\n",
    "                else:\n",
    "                    log_softmax = nn.LogSoftmax(-1)\n",
    "                    loss = -((log_softmax(logits) * labels).sum(-1)).mean()\n",
    "            elif self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        # add\n",
    "        loss_fct_rating = CrossEntropyLoss()\n",
    "        loss_rating = loss_fct_rating(\n",
    "            logits_rating.view(-1, self.rating_labels), rating.view(-1)\n",
    "        )\n",
    "        loss += loss_rating\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=16)\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch = [\n",
    "            {\n",
    "                \"input_ids\": f[\"input_ids\"],\n",
    "                \"token_type_ids\": f[\"token_type_ids\"],\n",
    "                \"attention_mask\": f[\"attention_mask\"],\n",
    "            }\n",
    "            for f in features\n",
    "        ]\n",
    "\n",
    "        batch = self.data_collator(batch)\n",
    "\n",
    "        labels = torch.tensor(\n",
    "            [\n",
    "                f[\"labels\"] if f[\"labels\"] is not None else random.choice([0, 1])\n",
    "                for f in features\n",
    "            ]\n",
    "        )\n",
    "        rating = torch.tensor(\n",
    "            [\n",
    "                f[\"rating\"]\n",
    "                if f[\"rating\"] is not None\n",
    "                else random.choice([0, 1, 2, 3, 4])\n",
    "                for f in features\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": batch[\"input_ids\"],\n",
    "            \"token_type_ids\": batch[\"token_type_ids\"],\n",
    "            \"attention_mask\": batch[\"attention_mask\"],\n",
    "            \"labels\": labels,\n",
    "            \"rating\": rating,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        input_ids = torch.tensor(item[\"input_ids\"])\n",
    "        token_type_ids = torch.tensor(item[\"token_type_ids\"])\n",
    "        attention_mask = torch.tensor(item[\"attention_mask\"])\n",
    "\n",
    "        if \"label\" in item:\n",
    "            label = torch.tensor(item[\"label\"])\n",
    "            rating = torch.tensor(item[\"Rating\"])\n",
    "        else:\n",
    "            label = None\n",
    "            rating = None\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": label,\n",
    "            \"rating\": rating,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of CustomDebertaSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'rating_classifier.bias', 'rating_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.add_tokens([AddedToken(\"\\n\", normalized=False)])\n",
    "tokenizer.add_tokens([AddedToken(\" \" * 2, normalized=False)])\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     MODEL_NAME, num_labels=NUM_LABELS\n",
    "# )\n",
    "model = CustomDebertaSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=NUM_LABELS, problem_type=\"single_label_classification\"\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=16)\n",
    "data_collator = CustomDataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a42d4811d054bce86104884ae73fe75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd66939445f742ef82342fd7be22bc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/11155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def tokenize(examples, max_token_length: int):\n",
    "#     separator = \" [SEP] \"\n",
    "\n",
    "#     joined_text = examples[\"Title\"] + separator + examples[\"Review Text\"]\n",
    "\n",
    "#     return tokenizer(\n",
    "#         joined_text,\n",
    "#         max_length=max_token_length,\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#     )\n",
    "\n",
    "\n",
    "# def tokenize(examples, max_token_length: int):\n",
    "#     return tokenizer(\n",
    "#         examples[\"Title\"],\n",
    "#         max_length=max_token_length,\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#     )\n",
    "\n",
    "\n",
    "# def tokenize(examples, max_token_length: int):\n",
    "#     return tokenizer(\n",
    "#         examples[\"Review Text\"],\n",
    "#         max_length=max_token_length,\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#     )\n",
    "\n",
    "\n",
    "def use_all_info(examples) -> str:\n",
    "    separator = \" [SEP] \"\n",
    "    return (\n",
    "        \"Review Text \"\n",
    "        + examples[\"Review Text\"]\n",
    "        + separator\n",
    "        + \"Title \"\n",
    "        + examples[\"Title\"]\n",
    "        + separator\n",
    "        + \"Positive Feedback Count \"\n",
    "        + str(examples[\"Positive Feedback Count\"])\n",
    "        + separator\n",
    "        + \"Age \"\n",
    "        + str(examples[\"Age\"])\n",
    "        + separator\n",
    "        + \"Division Name \"\n",
    "        + examples[\"Division Name\"]\n",
    "        + separator\n",
    "        + \"Department Name \"\n",
    "        + examples[\"Department Name\"]\n",
    "        + separator\n",
    "        + \"Class Name \"\n",
    "        + examples[\"Class Name\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def tokenize(examples, max_token_length: int):\n",
    "    text = use_all_info(examples)\n",
    "\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        max_length=max_token_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize,\n",
    "    batched=False,\n",
    "    fn_kwargs={\"max_token_length\": TRAINING_MAX_LENGTH},\n",
    "    num_proc=NUM_PROC,\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    tokenize,\n",
    "    batched=False,\n",
    "    fn_kwargs={\"max_token_length\": INFERENCE_MAX_LENGTH},\n",
    "    num_proc=NUM_PROC,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Review Text Adorable, well-made skirt! lined and very slimming. i had to size up b/c it runs a bit snug around the waist. however, it's worth it b/c this will match many long and short sleeve tops![SEP] Title 3-season skirt![SEP] Positive Feedback Count 4[SEP] Age 25[SEP] Division Name General[SEP] Department Name Bottoms[SEP] Class Name Skirts[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Review Text I love this skirt. it does run a little small, i ordered an extra small and it is tight at the waist, but still works. i have received more compliments on this skirt than maybe anything i have owned. i've been stopped at the grocery store by people telling me how much they like it. i also work with kids, and they all love it as well.[SEP] Title So happy i bought this skirt![SEP] Positive Feedback Count 0[SEP] Age 32[SEP] Division Name General[SEP] Department Name Bottoms[SEP] Class Name Skirts[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(test_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4259522825b14c2ea2560800485dce0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=8):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8384cc72901844d5ab64b3a43d5cf972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=8):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_train = train_dataset.filter(\n",
    "    lambda x: x[\"fold\"] != USE_FOLD, num_proc=NUM_PROC\n",
    ")\n",
    "filtered_valid = train_dataset.filter(\n",
    "    lambda x: x[\"fold\"] == USE_FOLD, num_proc=NUM_PROC\n",
    ")\n",
    "\n",
    "train_valid_dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": filtered_train,\n",
    "        \"valid\": filtered_valid,\n",
    "    }\n",
    ")\n",
    "\n",
    "del filtered_train, filtered_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'label', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name', 'fold', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 6666\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['index', 'Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'label', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name', 'fold', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3334\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    set(train_valid_dataset[\"train\"][\"index\"])\n",
    "    & set(train_valid_dataset[\"valid\"][\"index\"])\n",
    "    == set()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds_prob = softmax(predictions, axis=-1)\n",
    "    return {\"eval_roc_auc\": roc_auc_score(labels, preds_prob[:, 1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スケジューラの設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_PATH,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEP,\n",
    "    eval_accumulation_steps=GRAD_ACC_STEP,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    num_train_epochs=EPOCH,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=0.1,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=2,\n",
    "    seed=SEED,\n",
    "    metric_for_best_model=\"eval_roc_auc\",\n",
    "    greater_is_better=True,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    report_to=REPORT_TO,\n",
    "    run_name=EXP_NAME,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    "    # dataloader_num_workers=0,  # debug用\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    # train_dataset=train_valid_dataset[\"train\"],\n",
    "    train_dataset=CustomDataset(train_valid_dataset[\"train\"]),\n",
    "    # eval_dataset=train_valid_dataset[\"valid\"],\n",
    "    eval_dataset=CustomDataset(train_valid_dataset[\"valid\"]),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='208' max='208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [208/208 10:19, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.520000</td>\n",
       "      <td>1.525517</td>\n",
       "      <td>0.885698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.375500</td>\n",
       "      <td>1.164934</td>\n",
       "      <td>0.920910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.249600</td>\n",
       "      <td>1.044635</td>\n",
       "      <td>0.955004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.089400</td>\n",
       "      <td>1.049163</td>\n",
       "      <td>0.953659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.974400</td>\n",
       "      <td>1.006158</td>\n",
       "      <td>0.962547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.000400</td>\n",
       "      <td>0.969531</td>\n",
       "      <td>0.963146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.882700</td>\n",
       "      <td>0.942322</td>\n",
       "      <td>0.962014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.885900</td>\n",
       "      <td>0.939441</td>\n",
       "      <td>0.965554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.736600</td>\n",
       "      <td>0.937159</td>\n",
       "      <td>0.965185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    # モデルの学習\n",
    "    trainer.train()\n",
    "    # ログの保存に利用したストレージを削除\n",
    "    # os.system(f\"rm -rf {MODEL_OUTPUT_PATH}/checkpoint-*\")\n",
    "    # モデルの保存\n",
    "    trainer.save_model(MODEL_OUTPUT_PATH)\n",
    "else:\n",
    "    pass\n",
    "# else:\n",
    "#     # TRAINED_MODEL_PATHを用いて、学習済のモデルを読み込む\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#         TRAINED_MODEL_PATH,\n",
    "#         num_labels=NUM_LABELS,\n",
    "#     )\n",
    "\n",
    "#     args = TrainingArguments(\n",
    "#         \".\",\n",
    "#         per_device_eval_batch_size=4,\n",
    "#         report_to=\"none\",\n",
    "#         fp16=True,\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=args,\n",
    "#         data_collator=data_collator,\n",
    "#         tokenizer=tokenizer,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# valid_datasetの作成・保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds_prob = softmax(predictions, axis=-1)\n",
    "    return {\"eval_roc_auc\": roc_auc_score(labels, preds_prob[:, 1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f473eafc011246ebb3623eba6b2cced2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=8):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af2f7df18e042ef994748b1e4d3741c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/3334 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3982635b0b64985894760b99c719e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3334 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TRAININGをINFERRENCEでMAX_TOKENを変えるために、validを作り直す\n",
    "valid_dataset = train_dataset.filter(\n",
    "    lambda example: example[\"index\"] in train_valid_dataset[\"valid\"][\"index\"],\n",
    "    num_proc=NUM_PROC,\n",
    ")\n",
    "\n",
    "valid_dataset = valid_dataset.map(\n",
    "    tokenize,\n",
    "    batched=False,\n",
    "    fn_kwargs={\"max_token_length\": INFERENCE_MAX_LENGTH},\n",
    "    num_proc=NUM_PROC,\n",
    ")\n",
    "\n",
    "\n",
    "def add_valid_pred(example, idx, valid_pred):\n",
    "    example[\"valid_pred\"] = valid_pred[idx]\n",
    "    return example\n",
    "\n",
    "\n",
    "# valid_pred = softmax(trainer.predict(valid_dataset).predictions, axis=-1)\n",
    "valid_pred = softmax(trainer.predict(CustomDataset(valid_dataset)).predictions, axis=-1)\n",
    "\n",
    "np.save(f\"{MODEL_OUTPUT_PATH}/valid_prediction.npy\", valid_pred)\n",
    "\n",
    "valid_dataset = valid_dataset.map(\n",
    "    add_valid_pred, with_indices=True, fn_kwargs={\"valid_pred\": valid_pred[:, 1]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    valid_dataset.to_polars()\n",
    "    .select(pl.exclude(\"input_ids\", \"attention_mask\", \"token_type_ids\"))\n",
    "    .write_csv(f\"{MODEL_OUTPUT_PATH}/valid_dataset_{EXP_NAME}.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 混同行列の作成\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 混同行列を作成\n",
    "cm = confusion_matrix(valid_dataset[\"label\"], valid_pred.argmax(axis=1))\n",
    "\n",
    "# Seabornでヒートマップをプロット\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar=False,\n",
    "    xticklabels=[0, 1],\n",
    "    yticklabels=[0, 1],\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix Heatmap\")\n",
    "plt.savefig(f\"{MODEL_OUTPUT_PATH}/confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9301139772045591"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(valid_dataset[\"label\"], valid_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 0.9655486466715436\n"
     ]
    }
   ],
   "source": [
    "cv_score = roc_auc_score(valid_dataset[\"label\"], valid_pred[:, 1])\n",
    "print(f\"CV Score: {cv_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_textを保存\n",
    "with open(f\"{MODEL_OUTPUT_PATH}/cv_score.txt\", \"w\") as f:\n",
    "    f.write(str(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テストに対する計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4dac5add4f946d388604380a5102056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/11155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = test_dataset.map(\n",
    "    tokenize,\n",
    "    batched=False,\n",
    "    fn_kwargs={\"max_token_length\": INFERENCE_MAX_LENGTH},\n",
    "    num_proc=NUM_PROC,\n",
    ")\n",
    "\n",
    "\n",
    "def add_valid_pred(example, idx, valid_pred):\n",
    "    example[\"valid_pred\"] = valid_pred[idx]\n",
    "    return example\n",
    "\n",
    "\n",
    "# test_pred = softmax(trainer.predict(test_dataset).predictions, axis=-1)\n",
    "test_pred = softmax(trainer.predict(CustomDataset(test_dataset)).predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提出ファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pl.read_csv(f\"{DATA_PATH}/sample_submission.csv\")\n",
    "\n",
    "if DEBUG:\n",
    "    sample_submission = sample_submission.head(100)\n",
    "\n",
    "(\n",
    "    sample_submission.with_columns(\n",
    "        pl.Series(test_pred[:, 1]).alias(\"target\")\n",
    "    ).write_csv(f\"{MODEL_OUTPUT_PATH}/submission_{EXP_NAME}_cv{cv_score:.4f}.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWSへのアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/usr/bin/aws': No such file or directory\n",
      "rm: cannot remove '/usr/bin/aws_completer': No such file or directory\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 58.0M  100 58.0M    0     0   150M      0 --:--:-- --:--:-- --:--:--  150M\n",
      "You can now run: /usr/local/bin/aws --version\n",
      "upload: ../trained_models/e021-MaxPooling/checkpoint-168/config.json to s3://atmacup17/trained_model/e021-MaxPooling/checkpoint-168/config.json\n",
      "upload: ../trained_models/e021-MaxPooling/checkpoint-168/added_tokens.json to s3://atmacup17/trained_model/e021-MaxPooling/checkpoint-168/added_tokens.json\n",
      "upload: ../trained_models/e021-MaxPooling/checkpoint-168/special_tokens_map.json to s3://atmacup17/trained_model/e021-MaxPooling/checkpoint-168/special_tokens_map.json\n",
      "upload: ../trained_models/e021-MaxPooling/checkpoint-168/scheduler.pt to s3://atmacup17/trained_model/e021-MaxPooling/checkpoint-168/scheduler.pt\n",
      "upload: ../trained_models/e021-MaxPooling/added_tokens.json to s3://atmacup17/trained_model/e021-MaxPooling/added_tokens.json\n",
      "upload: ../trained_models/e021-MaxPooling/checkpoint-168/rng_state.pth to s3://atmacup17/trained_model/e021-MaxPooling/checkpoint-168/rng_state.pth\n",
      "upload: ../trained_models/e021-MaxPooling/checkpoint-168/tokenizer_config.json to s3://atmacup17/trained_model/e021-MaxPooling/checkpoint-168/tokenizer_config.json\n",
      "upload: ../trained_models/e021-MaxPooling/checkpoint-168/trainer_state.json to s3://atmacup17/trained_model/e021-MaxPooling/checkpoint-168/trainer_state.json\n",
      "upload: ../trained_models/e021-MaxPooling/checkpoint-168/training_args.bin to s3://atmacup17/trained_model/e021-MaxPooling/checkpoint-168/training_args.bin\n",
      "upload: ../trained_models/e021-MaxPooling/checkpoint-168/spm.model to s3://atmacup17/trained_model/e021-MaxPooling/checkpoint-168/spm.model\n",
      "upload: ../trained_models/e021-MaxPooling/config.json to s3://atmacup17/trained_model/e021-MaxPooling/config.json\n",
      "upload: ../trained_models/e021-MaxPooling/cv_score.txt to s3://atmacup17/trained_model/e021-MaxPooling/cv_score.txt\n",
      "upload: ../trained_models/e021-MaxPooling/confusion_matrix.png to s3://atmacup17/trained_model/e021-MaxPooling/confusion_matrix.png\n",
      "upload: ../trained_models/e021-MaxPooling/checkpoint-168/tokenizer.json to s3://atmacup17/trained_model/e021-MaxPooling/checkpoint-168/tokenizer.json\n",
      "upload: ../trained_models/e021-MaxPooling/special_tokens_map.json to s3://atmacup17/trained_model/e021-MaxPooling/special_tokens_map.json\n",
      "upload: ../trained_models/e021-MaxPooling/spm.model to s3://atmacup17/trained_model/e021-MaxPooling/spm.model\n",
      "upload: ../trained_models/e021-MaxPooling/submission_e021-MaxPooling_cv0.9655.csv to s3://atmacup17/trained_model/e021-MaxPooling/submission_e021-MaxPooling_cv0.9655.csv\n",
      "upload: ../trained_models/e021-MaxPooling/tokenizer_config.json to s3://atmacup17/trained_model/e021-MaxPooling/tokenizer_config.json\n",
      "upload: ../trained_models/e021-MaxPooling/training_args.bin to s3://atmacup17/trained_model/e021-MaxPooling/training_args.bin\n",
      "upload: ../trained_models/e021-MaxPooling/valid_dataset_e021-MaxPooling.csv to s3://atmacup17/trained_model/e021-MaxPooling/valid_dataset_e021-MaxPooling.csv\n",
      "upload: ../trained_models/e021-MaxPooling/tokenizer.json to s3://atmacup17/trained_model/e021-MaxPooling/tokenizer.json\n",
      "upload: ../trained_models/e021-MaxPooling/valid_prediction.npy to s3://atmacup17/trained_model/e021-MaxPooling/valid_prediction.npy\n",
      "upload: ../trained_models/e021-MaxPooling/checkpoint-168/model.safetensors to s3://atmacup17/trained_model/e021-MaxPooling/checkpoint-168/model.safetensors\n",
      "upload: ../trained_models/e021-MaxPooling/model.safetensors to s3://atmacup17/trained_model/e021-MaxPooling/model.safetensors\n",
      "upload: ../trained_models/e021-MaxPooling/checkpoint-168/optimizer.pt to s3://atmacup17/trained_model/e021-MaxPooling/checkpoint-168/optimizer.pt\n"
     ]
    }
   ],
   "source": [
    "# S3へのアップロード\n",
    "# if not DEBUG and UPLOAD_DATA_TO_S3:\n",
    "if UPLOAD_DATA_TO_S3:\n",
    "    # uninstall\n",
    "    !sudo rm /usr/bin/aws\n",
    "    !sudo rm /usr/bin/aws_completer\n",
    "    !sudo rm -rf /usr/local/aws-cli\n",
    "\n",
    "    # install\n",
    "    !curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "    !unzip -o -qq awscliv2.zip\n",
    "    !sudo ./aws/install --update\n",
    "\n",
    "    # upload\n",
    "    output_name = MODEL_OUTPUT_PATH.split(\"/\")[-1]\n",
    "    os.system(\n",
    "        f\"aws s3 cp --recursive {MODEL_OUTPUT_PATH} s3://{COMPETITION_NAME}/trained_model/{output_name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ダウンロード（参考）\n",
    "# !sudo rm /usr/bin/aws\n",
    "# !sudo rm /usr/bin/aws_completer\n",
    "# !sudo rm -rf /usr/local/aws-cli\n",
    "\n",
    "# !curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "# !unzip -o -qq awscliv2.zip\n",
    "# !sudo ./aws/install --update\n",
    "\n",
    "# !aws s3 cp --recursive s3://automated-essay-scoring/trained_model/e005-regression /notebooks/automated_essay_scoring/trained_models/e005-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36edc0aaa402402780c86cb4b7f23508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▂▂▂▁▁▁▁</td></tr><tr><td>eval/roc_auc</td><td>▁▄▇▇█████</td></tr><tr><td>eval/runtime</td><td>▃▅▅▃█▅▃▆▁</td></tr><tr><td>eval/samples_per_second</td><td>▆▄▄▆▁▄▆▃█</td></tr><tr><td>eval/steps_per_second</td><td>▆▄▄▆▁▄▆▃█</td></tr><tr><td>test/eval_roc_auc</td><td>█▁</td></tr><tr><td>test/loss</td><td>▁█</td></tr><tr><td>test/runtime</td><td>▁█</td></tr><tr><td>test/samples_per_second</td><td>█▁</td></tr><tr><td>test/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>█▆▃▂▃▆▇▃▂▂▂▁▂▂▂▂▃▃▁▃▂▂▂▂▂▁▂▁▁▁▁▂▁▁▂▂▂▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▂▃▅▆██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▅▅▅▇▄▃▄▃▃▃▃▂▂▃▃▃▃▂▃▂▃▁▂▁▁▂▂▁▁▂▁▁▁▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.93716</td></tr><tr><td>eval/roc_auc</td><td>0.96519</td></tr><tr><td>eval/runtime</td><td>16.5762</td></tr><tr><td>eval/samples_per_second</td><td>201.132</td></tr><tr><td>eval/steps_per_second</td><td>25.157</td></tr><tr><td>test/eval_roc_auc</td><td>0.49846</td></tr><tr><td>test/loss</td><td>5.69395</td></tr><tr><td>test/runtime</td><td>54.7921</td></tr><tr><td>test/samples_per_second</td><td>203.588</td></tr><tr><td>test/steps_per_second</td><td>25.46</td></tr><tr><td>total_flos</td><td>1752829966030848.0</td></tr><tr><td>train/epoch</td><td>3.99041</td></tr><tr><td>train/global_step</td><td>208</td></tr><tr><td>train/grad_norm</td><td>2.91318</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.7312</td></tr><tr><td>train_loss</td><td>1.09089</td></tr><tr><td>train_runtime</td><td>621.4493</td></tr><tr><td>train_samples_per_second</td><td>42.906</td></tr><tr><td>train_steps_per_second</td><td>0.335</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">e021-MaxPooling</strong> at: <a href='https://wandb.ai/sinchir0/atmacup17/runs/149dk0r3' target=\"_blank\">https://wandb.ai/sinchir0/atmacup17/runs/149dk0r3</a><br/> View project at: <a href='https://wandb.ai/sinchir0/atmacup17' target=\"_blank\">https://wandb.ai/sinchir0/atmacup17</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240830_063944-149dk0r3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if WANDB:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish Notebook!\n"
     ]
    }
   ],
   "source": [
    "print(\"finish Notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
